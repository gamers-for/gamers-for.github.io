# Game8 完全スクレイピング手順書

## 目的

Game8の特定ゲーム攻略セクション（例: スプラトゥーン3）の **全ページ** のraw HTMLを、
1ページも漏らさず取得する。

---

## 前提知識: Game8のURL構造

```
https://game8.jp/{game_slug}                ← ゲームトップページ（記事IDなし）
https://game8.jp/{game_slug}/{記事ID}       ← 個別記事ページ（数字のID）
https://game8.jp/{game_slug}/{英単語パス}   ← 一部の特殊ページ（例: /ranking）
```

- `{game_slug}` = ゲーム識別子（例: `splatoon3`, `genshin`, `pokemonsv`）
- `{記事ID}` = **多くは数字だが、英単語パスも存在する**（例: `ranking`）
- ゲーム内の全ページは必ず `/{game_slug}/` 配下にある
- ページ内リンクは絶対URL（`https://game8.jp/splatoon3/480285`）または相対URL（`/splatoon3/480285`）

---

## URL発見戦略: 2系統で漏れを潰す

**1系統だけでは不十分。** 2つの独立したソースからURLを収集し、和集合を取る。

| ソース | 強み | 弱み |
|--------|------|------|
| **sitemap.xml** | 全ページが機械的にリスト化されている | ゲーム単位のフィルタリングが必要 |
| **BFSクロール** | 実際にページが存在することを確認済み | 孤立ページ（どこからもリンクされていない）を発見できない |

**最終的なURL一覧 = sitemap発見URL ∪ BFS発見URL**

片方にしかないURLがあれば、それは片方だけでは漏れていた証拠。

---

## ステップ0: sitemap.xmlから全URL抽出（主軸）

### Game8のsitemap

```
https://game8.jp/sitemaps/sitemap.xml.gz
```

robots.txtで公開されている公式sitemap。全ページのURLが記載されている。

### 処理手順

```python
import gzip
import xml.etree.ElementTree as ET
import requests
import re

def get_urls_from_sitemap(game_slug):
    """sitemapから特定ゲームの全URLを抽出"""
    sitemap_url = "https://game8.jp/sitemaps/sitemap.xml.gz"

    # 1. sitemapダウンロード（gzip圧縮）
    resp = requests.get(sitemap_url, timeout=60)
    xml_data = gzip.decompress(resp.content)

    # 2. XMLパース
    root = ET.fromstring(xml_data)
    ns = {"sm": "http://www.sitemaps.org/schemas/sitemap/0.9"}

    # 3. sitemapインデックスの場合、子sitemapを再帰的に取得
    #    （Game8はインデックス形式の可能性あり）
    child_sitemaps = root.findall(".//sm:sitemap/sm:loc", ns)
    if child_sitemaps:
        # インデックス形式 → 子sitemapを全てダウンロードしてURLを収集
        all_urls = set()
        for child in child_sitemaps:
            child_url = child.text
            # 対象ゲームに関係しそうな子sitemapのみ取得
            child_resp = requests.get(child_url, timeout=60)
            if child_url.endswith(".gz"):
                child_xml = gzip.decompress(child_resp.content)
            else:
                child_xml = child_resp.content
            child_root = ET.fromstring(child_xml)
            for loc in child_root.findall(".//sm:url/sm:loc", ns):
                url = loc.text.strip()
                if is_target_url(url, game_slug):
                    all_urls.add(url)
        return all_urls

    # 4. 単一sitemapの場合
    urls = set()
    for loc in root.findall(".//sm:url/sm:loc", ns):
        url = loc.text.strip()
        if is_target_url(url, game_slug):
            urls.add(url)
    return urls
```

### 出力

```python
# 例: splatoon3の場合
sitemap_urls = get_urls_from_sitemap("splatoon3")
print(f"sitemap発見URL: {len(sitemap_urls)}件")
# → sitemap発見URL: 312件（推定）
```

**sitemapで見つかったURLをファイルに保存** → `_sitemap_urls.json`

---

## ステップ1: BFSクロールでURL発見 + ダウンロード（補完）

### 方針: ハードコードしない。自動発見する。

**絶対にURLを手動リストにしない。** 前回の失敗原因はURLをハードコードしたこと。
トップページを起点にリンクを再帰的にたどって全URLを自動収集する。

### 初期キュー = sitemapのURL全部 + トップページ

```python
# BFSの初期キューにsitemap結果を全て入れる
queue = list(sitemap_urls)
if top_url not in queue:
    queue.insert(0, top_url)
```

**こうすることで、sitemapの全ページを訪問しつつ、
各ページ内のリンクからsitemapに載っていないページも発見できる。**

### アルゴリズム

```
入力: game_slug, sitemap_urls
出力: 全ページのHTML + 全URL一覧

1. キュー = sitemap_urls のリスト + [トップページURL]
2. 発見済みURL = set(sitemap_urls)  ← sitemapのURLは最初から発見済み
3. 処理済みURL = set()
4. while キューが空でない:
     a. url = キューから1つ取り出す
     b. urlが処理済みならスキップ
     c. 処理済みに追加
     d. 保存済みファイルがあればダウンロードスキップ（リンク抽出用にファイル読込）
     e. なければGETダウンロード（5〜10秒ランダム待機後、リトライ付き）
     f. HTML内の全リンク（aタグのhref）を抽出
     g. 各リンクについて:
        - 絶対URLに正規化（フラグメント除去、クエリ除去）
        - is_target_url() で対象判定
        - 対象かつ未発見 → 発見済みに追加 + キューに追加
     h. HTMLをファイルに保存
5. 完了
```

### URLフィルタリング条件

```python
import re
from urllib.parse import urlparse, urljoin

def normalize_url(url, base_url="https://game8.jp"):
    """URLを正規化（フラグメント・クエリ除去、相対→絶対変換）"""
    url = urljoin(base_url, url)
    parsed = urlparse(url)
    # フラグメントとクエリを除去
    normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
    # 末尾スラッシュを統一（除去）
    return normalized.rstrip("/")

def is_target_url(url, game_slug):
    """対象URLかどうか判定"""
    url = normalize_url(url)

    # パターン1: トップページ
    if url == f"https://game8.jp/{game_slug}":
        return True

    # パターン2: 記事ページ（数字ID）
    if re.match(rf"^https://game8\.jp/{game_slug}/\d+$", url):
        return True

    # パターン3: 英単語パスのページ（例: /ranking, /category/xxx）
    #   ただし以下は除外:
    #   - /user/  （ユーザーページ）
    #   - /search/ （検索結果）
    #   - クエリパラメータ付き（正規化で既に除去済み）
    if re.match(rf"^https://game8\.jp/{game_slug}/[a-zA-Z][\w\-/]*$", url):
        # 除外パターン
        exclude = ["/user/", "/search/", "/preview/", "/edit/", "/draft/"]
        if not any(ex in url for ex in exclude):
            return True

    return False
```

**除外するURL:**
- フラグメント（`#hl_1` 等）→ 正規化で自動除去
- クエリパラメータ付き（`?page=`, `?ref=` 等）→ 正規化で自動除去
- 他ゲームのURL（`/genshin/` 等）→ game_slug不一致で除外
- 外部リンク（`game8.jp` 以外のドメイン）→ ドメインチェックで除外
- `/user/`, `/search/` 等の非コンテンツURL → 除外リストで除外

---

## ステップ2: ダウンロード実行

### リクエスト設定

```python
import random
import time
import requests

SESSION = requests.Session()  # セッション維持でCookie管理
SESSION.headers.update({
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) "
                  "Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "ja,en;q=0.9",
    "Accept-Encoding": "gzip, deflate, br",
    "Referer": "https://game8.jp/",
})
TIMEOUT = 30  # 秒
```

### 待機ルール（最重要）

```python
def wait():
    """リクエスト間の待機。5〜10秒のランダム"""
    delay = random.uniform(5.0, 10.0)
    time.sleep(delay)
```

- **毎回のリクエスト前に必ず `wait()` を呼ぶ**
- 連続エラー時はさらに長く待つ（後述のリトライ参照）

### リトライ（指数バックオフ）

```python
def fetch_with_retry(url, max_retries=3):
    """リトライ付きGET。指数バックオフ。"""
    for attempt in range(max_retries + 1):
        if attempt > 0:
            backoff = 30 * (2 ** (attempt - 1))  # 30秒, 60秒, 120秒
            print(f"  リトライ {attempt}/{max_retries} - {backoff}秒待機...")
            time.sleep(backoff)

        try:
            resp = SESSION.get(url, timeout=TIMEOUT)

            if resp.status_code == 200:
                return resp.text
            elif resp.status_code == 429:  # Too Many Requests
                print(f"  レート制限検出 (429) - 長めに待機")
                time.sleep(120)  # 2分待機
                continue
            elif resp.status_code == 403:  # Forbidden
                print(f"  アクセス拒否 (403) - 長めに待機")
                time.sleep(180)  # 3分待機
                continue
            elif resp.status_code >= 500:  # サーバーエラー
                print(f"  サーバーエラー ({resp.status_code})")
                continue
            else:
                print(f"  HTTP {resp.status_code}: {url}")
                return None  # 404等はリトライしない

        except requests.exceptions.Timeout:
            print(f"  タイムアウト")
            continue
        except requests.exceptions.ConnectionError:
            print(f"  接続エラー")
            time.sleep(60)
            continue

    print(f"  ✗ 最終失敗: {url}")
    return None
```

### ファイル保存

```python
def url_to_filename(url, game_slug):
    """URLからファイル名を生成。シンプルで一意。"""
    path = urlparse(url).path  # 例: /splatoon3/478553
    suffix = path.replace(f"/{game_slug}", "").strip("/")
    if not suffix:
        return "top.html"
    # スラッシュを_に（サブパス対応）
    safe = suffix.replace("/", "_")
    return f"{safe}.html"

def save_html(html, url, game_slug, output_dir):
    """HTMLをファイルに保存"""
    filename = url_to_filename(url, game_slug)
    filepath = os.path.join(output_dir, filename)
    with open(filepath, "w", encoding="utf-8") as f:
        f.write(html)
    return filepath
```

**ファイル名の例:**
- `https://game8.jp/splatoon3` → `top.html`
- `https://game8.jp/splatoon3/478553` → `478553.html`
- `https://game8.jp/splatoon3/ranking` → `ranking.html`

前回の失敗: 武器名をファイル名に入れたため、特殊文字（`/`, `・`）の処理で不一致が発生。
URLパスから機械的に生成すれば一意で確実。

---

## ステップ3: レジューム（中断→再開）対応

スクレイピングが途中で止まっても、最初からやり直さなくて済むようにする。

```python
def get_already_saved(output_dir):
    """保存済みファイル名セットを返す"""
    saved = set()
    for f in os.listdir(output_dir):
        if f.endswith(".html"):
            saved.add(f)
    return saved
```

クロール時:
```python
already_saved = get_already_saved(output_dir)
filename = url_to_filename(url, game_slug)

if filename in already_saved:
    print(f"  スキップ（保存済み）: {filename}")
    # ただしリンク抽出のためにファイルを読み込んでリンクは収集する
    with open(os.path.join(output_dir, filename)) as f:
        html = f.read()
    # → リンク抽出処理へ（新規URLを発見するため）
else:
    wait()
    html = fetch_with_retry(url)
    if html:
        save_html(html, url, game_slug, output_dir)
```

**重要: 保存済みファイルもリンク抽出は行う。**
新規URLの発見を止めないため。ダウンロードだけスキップする。

### 進捗ファイル

```python
def save_progress(output_dir, discovered, processed, failed):
    """進捗をJSONに保存。中断時の再開に使用"""
    progress = {
        "discovered": sorted(discovered),
        "processed": sorted(processed),
        "failed": sorted(failed),
        "timestamp": datetime.now().isoformat(),
    }
    with open(os.path.join(output_dir, "_progress.json"), "w") as f:
        json.dump(progress, f, ensure_ascii=False, indent=2)
```

50ページごと、またはエラー発生時に `_progress.json` を保存する。
再開時は `_progress.json` から `processed` と `failed` を読み込んでスキップ/リトライ。

---

## ステップ4: 完了検証（3種 + クロスチェック）

スクレイピング完了後、以下の検証を必ず行う。

### 検証1: 発見URL vs 保存ファイル

```python
def verify_completeness(discovered_urls, output_dir, game_slug):
    """発見した全URLに対応するHTMLファイルが存在するか確認"""
    saved = get_already_saved(output_dir)
    missing = []
    for url in discovered_urls:
        filename = url_to_filename(url, game_slug)
        if filename not in saved:
            missing.append(url)

    if missing:
        print(f"✗ 未取得ページ: {len(missing)}件")
        for url in missing:
            print(f"  - {url}")
    else:
        print(f"✓ 全ページ取得完了: {len(saved)}件")
    return missing
```

### 検証2: ファイルサイズチェック

```python
def verify_file_sizes(output_dir, min_size_bytes=10000):
    """異常に小さいファイル（不完全な可能性）を検出"""
    suspicious = []
    for f in os.listdir(output_dir):
        if not f.endswith(".html"):
            continue
        size = os.path.getsize(os.path.join(output_dir, f))
        if size < min_size_bytes:
            suspicious.append((f, size))

    if suspicious:
        print(f"⚠ 異常に小さいファイル: {len(suspicious)}件")
        for fname, size in suspicious:
            print(f"  - {fname}: {size} bytes")
    return suspicious
```

### 検証3: リンク網羅性チェック

```python
def verify_link_coverage(output_dir, game_slug):
    """全保存済みHTML内のリンクが、全て保存済みであることを確認"""
    saved = get_already_saved(output_dir)
    all_links_found = set()

    for f in os.listdir(output_dir):
        if not f.endswith(".html"):
            continue
        with open(os.path.join(output_dir, f)) as fh:
            html = fh.read()
        links = extract_links(html, game_slug)
        all_links_found.update(links)

    missing_filenames = all_links_found - saved
    if missing_filenames:
        print(f"✗ HTML内で参照されているが未取得のページ: {len(missing_filenames)}件")
        for fname in sorted(missing_filenames):
            print(f"  - {fname}")
    else:
        print(f"✓ リンク網羅性OK: 全リンク先が取得済み")
    return missing_filenames
```

### 検証4: sitemap vs BFS クロスチェック（新規追加）

```python
def cross_check(sitemap_urls, bfs_urls, game_slug):
    """2つのソースで発見したURLを比較"""
    sitemap_set = set(normalize_url(u) for u in sitemap_urls)
    bfs_set = set(normalize_url(u) for u in bfs_urls)

    only_sitemap = sitemap_set - bfs_set
    only_bfs = bfs_set - sitemap_set
    both = sitemap_set & bfs_set

    print(f"  両方で発見: {len(both)}件")

    if only_sitemap:
        print(f"  ⚠ sitemapのみ（BFSで未発見 = 孤立ページの可能性）: {len(only_sitemap)}件")
        for url in sorted(only_sitemap):
            print(f"    - {url}")

    if only_bfs:
        print(f"  ⚠ BFSのみ（sitemapに未掲載）: {len(only_bfs)}件")
        for url in sorted(only_bfs):
            print(f"    - {url}")

    if not only_sitemap and not only_bfs:
        print(f"  ✓ 完全一致: 両ソースで同じURL集合")

    return sitemap_set | bfs_set  # 和集合が最終的なURL一覧
```

**検証で未取得が見つかった場合 → そのURLを追加取得する。**
これを未取得が0件になるまで繰り返す。

---

## ステップ5: メタデータ保存

スクレイピング完了後、以下のJSONを保存する。

```python
metadata = {
    "game_slug": game_slug,
    "scrape_date": datetime.now().isoformat(),
    "total_pages": len(saved_files),
    "url_sources": {
        "sitemap": len(sitemap_urls),
        "bfs_crawl": len(bfs_urls),
        "union": len(all_urls),
    },
    "top_url": f"https://game8.jp/{game_slug}",
    "all_urls": {
        url_to_filename(url, game_slug): url
        for url in sorted(all_urls)
    },
    "failed_urls": sorted(failed_urls),
}
```

`_metadata.json` として保存。後続の処理で使用する。

---

## 完成スクリプトの全体構造

```
scrape_game8_complete.py <game_slug>
│
├── Phase 0: sitemap.xml.gzダウンロード → 対象ゲームのURL抽出
│   └── 出力: sitemap_urls（set）
│
├── Phase 1: BFSクロール + ダウンロード
│   ├── 初期キュー = sitemap_urls + トップページ
│   ├── BFS（幅優先探索）でリンクをたどる
│   ├── 対象URLのみフィルタリング（数字ID + 英単語パス）
│   ├── 保存済みはダウンロードスキップ（リンク抽出は実行）
│   ├── 各リクエスト前に5〜10秒ランダム待機
│   ├── 失敗時3回リトライ（指数バックオフ: 30秒→60秒→120秒）
│   ├── 429/403検出時は2〜3分待機
│   ├── 50件ごとに進捗ファイル保存
│   └── 出力: bfs_urls（set）+ HTMLファイル群
│
├── Phase 2: 検証
│   ├── 検証1: 発見URL vs 保存ファイル照合
│   ├── 検証2: ファイルサイズ異常チェック（10KB未満を検出）
│   ├── 検証3: リンク網羅性チェック（HTML内リンク先が全て保存済みか）
│   └── 検証4: sitemap vs BFS クロスチェック
│
├── Phase 3: 補完
│   ├── 検証で漏れが見つかったURLを追加ダウンロード
│   └── 再検証 → 0件になるまでループ（最大3回）
│
└── Phase 4: メタデータ保存
    └── _metadata.json 出力
```

---

## 禁止事項

1. **URLをハードコードしない** → 必ずsitemap + BFS自動発見
2. **sitemapを無視しない** → BFSだけでは孤立ページを見逃す
3. **リトライなしにしない** → 必ず3回リトライ + 指数バックオフ
4. **検証をスキップしない** → 4種の検証を全て実行
5. **待機を短くしない** → 最低5秒、エラー時は30秒以上
6. **ファイル名に日本語を使わない** → URLパスから機械的に生成
7. **クロールを1階層で止めない** → 発見した全ページからさらにリンクをたどる
8. **URLフィルタを数字のみにしない** → 英単語パスのページも含める

---

## 実行例

```bash
python3 scrape_game8_complete.py splatoon3
```

```
=== Game8 完全スクレイピング: splatoon3 ===

Phase 0: sitemap解析
  sitemap.xml.gzダウンロード完了 (2.1MB)
  splatoon3対象URL: 298件発見
  _sitemap_urls.json 保存完了

Phase 1: BFSクロール + ダウンロード
  初期キュー: 299件（sitemap 298 + トップ 1）
  [1/299] top.html ← https://game8.jp/splatoon3 (4,459行)
  待機 7.3秒...
  [2/299] 477903.html ← https://game8.jp/splatoon3/477903 (3,102行)
  待機 5.8秒...
  [50/299] 進捗保存...
  ...
  [299/299] 697757.html ← https://game8.jp/splatoon3/697757 (1,920行)
  BFS中に追加発見: 14件（sitemapに未掲載）
  [300/313] ...
  [313/313] クロール完了

Phase 2: 検証
  検証1: 発見URL 313件 vs 保存ファイル 313件 → ✓ 一致
  検証2: ファイルサイズ異常 → ✓ 0件
  検証3: リンク網羅性 → ✓ 全リンク先取得済み
  検証4: sitemap(298) vs BFS(313) クロスチェック
    両方で発見: 298件
    BFSのみ: 15件（sitemap未掲載だがリンクは存在）
    sitemapのみ: 0件
    → ✓ 和集合 313件 全て取得済み

Phase 3: 補完
  補完不要（全ページ取得済み）

Phase 4: メタデータ保存
  _metadata.json 保存完了

=== 完了: 313ページ取得（所要時間: 約42分） ===
```

---

## まとめ: なぜこれで漏れないのか

| 想定されるリスク | 対策 |
|-----------------|------|
| URLハードコード → カテゴリ丸ごと漏れ | sitemap + BFS自動発見 |
| BFSだけ → 孤立ページを見逃す | sitemapで全ページを補完 |
| sitemapだけ → 未掲載ページを見逃す | BFSクロールで補完 |
| 数字IDのみフィルタ → `/ranking`等が漏れ | 英単語パスも対象に含める |
| レート制限 → 後半が全滅 | 5〜10秒待機 + 3回リトライ + 指数バックオフ |
| 中断 → 最初からやり直し | レジューム機能 + 進捗ファイル |
| 完了チェックなし → 欠落に気づかない | 4種の検証 + 補完ループ |
| ファイル名不一致 → 照合不能 | URLパスから機械的に生成 |
